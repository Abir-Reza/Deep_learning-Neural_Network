{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\abirc\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-790b92e68e4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m### -----------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Flatten\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, Adagrad, Nadam, RMSprop, schedules\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "### -----------\n",
    "# configuration\n",
    "### -----------\n",
    "\n",
    "img_size = 160\n",
    "batch_size = 32\n",
    "shuffle_buffer_size = 1000\n",
    "learning_rate = 0.0001\n",
    "num_initial_epochs = 10\n",
    "fine_tune_at_layer = 100 # layer number after which fine-tuning should be performed (layers until this layer number are frozen)\n",
    "num_fine_tuning_epochs = 10\n",
    "num_classes = 1 # cats_vs_dogs has 1 class (i.e., 2 classes), imagenette has 10 classes, caltech101 has 101 classes, caltech_birds 200 classes\n",
    "\n",
    "img_shape = (img_size, img_size, 3)\n",
    "total_epochs = num_initial_epochs + num_fine_tuning_epochs\n",
    "\n",
    "### -------\n",
    "# load data\n",
    "### -------\n",
    "\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split = ['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info = True,\n",
    "    as_supervised = True)\n",
    "\n",
    "print(raw_train)\n",
    "print(raw_validation)\n",
    "print(raw_test)\n",
    "\n",
    "# show the first two images and labels from the training set\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "for image, label in raw_train.take(2):\n",
    "  plt.figure()\n",
    "  plt.imshow(image)\n",
    "  plt.title(get_label_name(label))\n",
    "\n",
    "###------------- some statistics over training data ---------------------\n",
    "\n",
    "image_list = []\n",
    "label_list = []\n",
    "\n",
    "for images, labels in raw_train.take(-1):\n",
    "  image_list.append(images.numpy())\n",
    "  label_list.append(labels.numpy())\n",
    "\n",
    "print(\"Number of training images: %d\" % len(image_list))\n",
    "print(\"Number of training labels: %d\" % len(label_list))\n",
    "print(\"Shape of first training image: %s\" % str(image_list[0].shape))\n",
    "print(\"Shape of second training image: %s\" % str(image_list[1].shape))\n",
    "\n",
    "# calculate some statistics over training images (minimal / maximal pixel value, mean pixel dimensions)\n",
    "min_pixel_value_per_image = np.zeros(len(image_list))\n",
    "max_pixel_value_per_image = np.zeros(len(image_list))\n",
    "dim_per_image = np.zeros((len(image_list),3))\n",
    "\n",
    "for i in range(len(image_list)):\n",
    "  min_pixel_value_per_image[i] = np.min(image_list[i])\n",
    "  max_pixel_value_per_image[i] = np.max(image_list[i])\n",
    "  dim_per_image[i] = image_list[i].shape\n",
    "\n",
    "print(\"minimal pixel value in training data: %f\" % np.min(min_pixel_value_per_image) )\n",
    "print(\"maximal pixel value in training data: %f\" % np.max(max_pixel_value_per_image) )\n",
    "print(\"mean pixel dimension in training data: %s\" % np.mean(dim_per_image, axis=0))\n",
    "print(\"std. dev. of mean pixel dimension in training data: %s\" % np.std(dim_per_image, axis=0, ddof=1))\n",
    "\n",
    "# statistics over labels:\n",
    "labels_numpy = np.array([label_list])\n",
    "print(\"minimal label in training data: %d\" % np.min(labels_numpy))\n",
    "print(\"maximal label in training data: %d\" % np.max(labels_numpy))\n",
    "print(\"number of non-zero labels in training data: %d\" % np.count_nonzero(labels_numpy))\n",
    "\n",
    "\n",
    "###-----------\n",
    "# process data\n",
    "###-----------\n",
    "\n",
    "# format images for the task, using the tf.image module:\n",
    "# resize the images to a fixed input size (img_size x img_size), \n",
    "# and rescale the input channels to a range of [-1,1]\n",
    "# the resize method by default does not preserve aspect ratio; \n",
    "# this may result in resized images being distorted\n",
    "# alternatively, you may use the resize_with_pad method\n",
    "# or set the parameter preserve_aspect_ratio=True in the resize method.\n",
    "# see also https://www.tensorflow.org/api_docs/python/tf/image/resize#used-in-the-notebooks_1\n",
    "def format_example(image, label):\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  image = (image/127.5) - 1 # maximal pixel value: 255, so rescaling leads to a range of [-1, 1]\n",
    "  image = tf.image.resize(image, (img_size, img_size))\n",
    "  return image, label\n",
    "\n",
    "# apply formatting function to each item in the dataset using the map method\n",
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)\n",
    "\n",
    "# shuffle and batch the data\n",
    "train_batches = train.shuffle(shuffle_buffer_size).batch(batch_size)\n",
    "validation_batches = validation.batch(batch_size)\n",
    "test_batches = test.batch(batch_size)\n",
    "\n",
    "# inspect a batch of data\n",
    "for image_batch, label_batch in train_batches.take(1): \n",
    "  pass\n",
    "\n",
    "print(\"shape of first training batch: %s\" % image_batch.shape)\n",
    "\n",
    "### ----------\n",
    "# create model\n",
    "### ----------\n",
    "\n",
    "# get base model from the pre-trained model MobileNetV2\n",
    "# this model contains the layers until the \"bottleneck\" (i.e. the layers before the fully connected layers)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=img_shape, \n",
    "                                               include_top=False, weights='imagenet')\n",
    "\n",
    "# let's see what this base model does to an input batch of size (batch_size, img_size, img_size, 3), e.g. (32, 160, 160, 3):\n",
    "feature_batch = base_model(image_batch)\n",
    "print(\"Shape of one feature batch: %s\" % str(feature_batch.shape) )\n",
    "\n",
    "\n",
    "# freeze the base_model (i.e., the convolutional feature extractor)\n",
    "base_model.trainable = False\n",
    "\n",
    "# print base model summary\n",
    "print(\"Base model:\")\n",
    "base_model.summary()\n",
    "\n",
    "# add a classification head via global average pooling, a fully connected layer and an output\n",
    "global_average_layer = GlobalAveragePooling2D()\n",
    "# just for information: shape of one feature batch after global average pooling \n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(\"Shape of one feature batch after global average pooling: %s\" % str(feature_batch_average.shape) )\n",
    "\n",
    "# use a fully connected layer to convert these features into a single prediction per image\n",
    "# remember that there are only 2 classes in the 'cats_vs_dogs' data set so a single output is enough\n",
    "# we do not need a sigmoid activation function because the prediction will be treated as logit \n",
    "# (i.e., raw number): Positive numbers predict class 1, negative numbers predict class 0.\n",
    "prediction_layer = Dense(num_classes) \n",
    "# just for information: shape of one feature batch after prediction layer \n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(\"Shape of one feature batch after prediction layer: %s\" % str(prediction_batch.shape) )\n",
    "\n",
    "# now put together the base model (i.e., feature extractor) and the two prediction layers \n",
    "# using a Sequential model\n",
    "model = Sequential([base_model, global_average_layer, prediction_layer])\n",
    "\n",
    "opt = RMSprop(learning_rate=learning_rate)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "print(\"Final model (frozen base model with trainable classification head):\")\n",
    "model.summary()\n",
    "\n",
    "# evaluate initial model\n",
    "loss0_val, accuracy0_val = model.evaluate(validation_batches)\n",
    "print(\"initial loss on validation data: %f\" % loss0_val)\n",
    "print(\"initial accuracy on validation data: %f\" %accuracy0_val)\n",
    "\n",
    "loss0_test, accuracy0_test = model.evaluate(test_batches)\n",
    "print(\"initial loss on test data: %f\" % loss0_test)\n",
    "print(\"initial accuracy on test data: %f\" % accuracy0_test)\n",
    "\n",
    "\n",
    "### ---------\n",
    "# train model\n",
    "### ---------\n",
    "\n",
    "history = model.fit(train_batches, epochs=num_initial_epochs, validation_data=validation_batches)\n",
    "\n",
    "# plot training and validation loss and accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# evaluate initially trained model\n",
    "loss1_val, accuracy1_val = model.evaluate(validation_batches)\n",
    "print(\"loss on validation data after initial training: %f\" % loss1_val)\n",
    "print(\"accuracy on validation data after initial training: %f\" % accuracy1_val)\n",
    "\n",
    "loss1_test, accuracy1_test = model.evaluate(test_batches)\n",
    "print(\"loss on test data after initial training: %f\" % loss1_test)\n",
    "print(\"accuracy on test data after initial training: %f\" % accuracy1_test)\n",
    "\n",
    "\n",
    "### ---------\n",
    "# Fine-tuning\n",
    "### ---------\n",
    "\n",
    "# should only be done after training of the classification head where the base_model is frozen (not trainable)!!!\n",
    "# un-freeze the top layers of the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# print number of layers of the base model\n",
    "print(\"Number of layers in the base model: %d\" % len(base_model.layers) )\n",
    "\n",
    "# freeze all the layers before the 'fine_tune_at_layer' layer\n",
    "for layer in base_model.layers[:fine_tune_at_layer]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# compile the model again, using a much lower learning rate\n",
    "opt = RMSprop(learning_rate=learning_rate/10.0)\n",
    "model.compile(optimizer=opt, loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "\n",
    "print(\"Final model (partly frozen base model with trainable classification head):\")\n",
    "model.summary()\n",
    "print(\"Number of trainable layers: %d\" % len(model.trainable_variables))\n",
    "\n",
    "# fine-tune model\n",
    "history_fine_tuning = model.fit(train_batches, epochs = total_epochs,\n",
    "                                initial_epoch = history.epoch[-1], validation_data=validation_batches )\n",
    "\n",
    "# plot training and validation loss and accuracy\n",
    "acc += history_fine_tuning.history['accuracy']\n",
    "val_acc += history_fine_tuning.history['val_accuracy']\n",
    "\n",
    "loss += history_fine_tuning.history['loss']\n",
    "val_loss += history_fine_tuning.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([num_initial_epochs-1,num_initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([num_initial_epochs-1,num_initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "# evaluate fine-tuned  model\n",
    "loss2_val, accuracy2_val = model.evaluate(validation_batches)\n",
    "print(\"loss on validation data after fine-tuning: %f\" % loss2_val)\n",
    "print(\"accuracy on validation data after fine-tuning: %f\" % accuracy2_val)\n",
    "\n",
    "loss2_test, accuracy2_test = model.evaluate(test_batches)\n",
    "print(\"loss on test data after fine-tuning: %f\" % loss2_test)\n",
    "print(\"accuracy on test data after fine-tuning: %f\" % accuracy2_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
